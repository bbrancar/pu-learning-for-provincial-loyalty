{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drB_NDLLhob8"
      },
      "source": [
        "#Adaptation of TEDn implementation for Positive-Unlabeled Learning, adapted from original implementation:\n",
        "\n",
        "https://github.com/acmi-lab/pu_learning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXKFgWJdUo3Z"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkeIsrKhVWnt"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define model storage directory and data location:\n",
        "experiment_dir = 'MODEL STORAGE DIR'\n",
        "data_dir = 'DATA LOCATION'"
      ],
      "metadata": {
        "id": "UCzRT_hKSiR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdKvWPhbgidL"
      },
      "source": [
        "# Model helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwMGlOKHiqDe"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
        "\n",
        "\n",
        "class AutoClassifier(AutoModelForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "      super().__init__(config)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \n",
        "        input_ids = x[:, :, 0]\n",
        "        attention_mask = x[:, :, 1]\n",
        "\n",
        "        outputs = super().__call__(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )[0]\n",
        "        return outputs\n",
        "\n",
        "def init_classification_model(model_name):\n",
        "    config_model = AutoConfig.from_pretrained(model_name)\n",
        "    config_model.num_labels = 2\n",
        "    config_model.max_len = 510\n",
        "    transformer_model = AutoClassifier.from_pretrained(model_name, config=config_model)\n",
        "\n",
        "    return transformer_model\n",
        "\n",
        "def get_model(model_type, input_dim=None): \n",
        "\n",
        "    if model_type == \"Roberta\":\n",
        "        net = init_classification_model('hfl/chinese-roberta-wwm-ext')\n",
        "        return net \n",
        "    else:\n",
        "        print(\"Must implement model if model other than Roberta\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtpgKddwjUh1"
      },
      "source": [
        "# Datahelper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGEGacsIbP-P"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizerFast, DistilBertTokenizerFast, AutoTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def read_tifa_split(df):\n",
        "    \n",
        "    train_texts = []\n",
        "    test_texts = []\n",
        "    train_labels = []\n",
        "    test_labels = []\n",
        "\n",
        "    tifa = pd.read_csv(df)\n",
        "\n",
        "    #Hard coded positive and unknown sample sizes for my use case\n",
        "    tifa_ones = tifa[tifa.hu_tifa == 1].sample(19000)\n",
        "    tifa_zeros = tifa[tifa.hu_tifa == 0].sample(97000)\n",
        "    \n",
        "    tifa = tifa_ones.append(tifa_zeros).reset_index(drop=True)\n",
        "\n",
        "    X, y = tifa['para_split'].tolist(), tifa['hu_tifa'].tolist()\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def getBertTokenizer(model):\n",
        "\n",
        "    if model == 'hfl/chinese-roberta-wwm-ext':\n",
        "        tokenizer = AutoTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext', add_prefix_space=True)\n",
        "    else:\n",
        "        raise ValueError(f'Model: {model} not recognized.')\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "def initialize_bert_transform(net):\n",
        "    # assert 'bert' in config.model\n",
        "    # assert config.max_token_length is not None\n",
        "\n",
        "    tokenizer = getBertTokenizer(net)\n",
        "    \n",
        "    def transform(text):\n",
        "        tokens = tokenizer(\n",
        "            text,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=510)\n",
        "        \n",
        "        if net == 'hfl/chinese-roberta-wwm-ext':\n",
        "            x = np.stack(\n",
        "                (tokens['input_ids'],\n",
        "                 tokens['attention_mask'],\n",
        "                 tokens['token_type_ids']),\n",
        "                axis=2)\n",
        "        return x\n",
        "    \n",
        "    return transform\n",
        "\n",
        "class TiFaData(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, labels, transform):\n",
        "        labels = np.array(labels)\n",
        "        \n",
        "        encodings = transform(data)\n",
        "\n",
        "        p_data_idx = np.where(labels==1)[0]\n",
        "        u_data_idx = np.where(labels==0)[0]\n",
        "        \n",
        "        self.p_data = encodings[p_data_idx, :, :]\n",
        "        self.u_data = encodings[u_data_idx, :, :]\n",
        "\n",
        "        self.labels = labels\n",
        "\n",
        "        self.transform = None\n",
        "        self.target_transform = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neB_MPzmjYbX"
      },
      "source": [
        "# Get data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wpRC_qKBIY4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.utils import data\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# New get_dataset func:\n",
        "class PosData(torch.utils.data.Dataset): \n",
        "    def __init__(self, transform=None, target_transform=None, data=None, \\\n",
        "            index=None):\n",
        "        \n",
        "        #None for IMDb\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        #The data\n",
        "        self.data=data\n",
        "\n",
        "        #Filler for labs\n",
        "        self.targets = np.zeros(data.shape[0], dtype= np.int_)\n",
        "        self.data_type = data_type\n",
        "        self.index = index\n",
        "\n",
        "    def __len__(self): \n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        index, txt, target = self.index[idx],  self.data[idx], self.targets[idx]\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "    \n",
        "        return index, txt, target\n",
        "    \n",
        "\n",
        "class UnlabelData(torch.utils.data.Dataset): \n",
        "    def __init__(self, transform=None, target_transform=None, \\\n",
        "            u_data=None, index=None):\n",
        "        \n",
        "        #These are non for IMDb\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "        #Pull together pos and negatives into single unlabeled\n",
        "        self.data= u_data\n",
        "        \n",
        "        #So it looks like targets are pos = 0, neg = 1\n",
        "        self.targets = np.ones(u_data.shape[0], dtype= np.int_)\n",
        "\n",
        "        self.data_type = data_type\n",
        "        self.index = index\n",
        "\n",
        "    def __len__(self): \n",
        "        return len(self.targets)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        index, img, target = self.index[idx],  self.data[idx], self.targets[idx]\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "    \n",
        "        return index, img, target\n",
        "\n",
        "def get_PUDataSplits(data_obj): \n",
        "\n",
        "\n",
        "    pos_data = data_obj.p_data\n",
        "    u_data = data_obj.u_data\n",
        "\n",
        "    unlabel_size = u_data.shape[0]\n",
        "    pos_size = pos_data.shape[0]\n",
        "\n",
        "    #These are then passed to the PosData and UnlabeledData functions:\n",
        "    return PosData(transform=data_obj.transform, \\\n",
        "                target_transform=data_obj.target_transform, \\\n",
        "                data=pos_data, index=np.array(range(pos_size))), \\\n",
        "            UnlabelData(transform=data_obj.transform, \\\n",
        "                target_transform=data_obj.target_transform, \\\n",
        "                u_data = u_data,  \\\n",
        "                index=np.array(range(unlabel_size))), \\\n",
        "                pos_size, \\\n",
        "                unlabel_size\n",
        "            \n",
        "def get_dataset(data_dir, data_type, net_type, device, batch_size): \n",
        "\n",
        "    p_trainloader=None\n",
        "    u_trainloader=None\n",
        "    p_validloader=None\n",
        "    u_validloader=None\n",
        "    net=None\n",
        "    X=None\n",
        "    Y=None\n",
        "\n",
        "    if data_type==\"TiFa_BERT\": \n",
        "\n",
        "        #Pull indata\n",
        "        train_texts, test_texts, train_labels, test_labels = read_tifa_split(data_dir)\n",
        "\n",
        "        #Bert transform... initializes tokenizer to be passed to roberta\n",
        "        transform = initialize_bert_transform('hfl/chinese-roberta-wwm-ext')\n",
        "\n",
        "        #Transforms data using tokenizer, also separates our pos vs. unlabled data and stores these in self\n",
        "        train_dataset = TiFaData(train_texts, train_labels, transform=transform)\n",
        "        test_dataset = TiFaData(test_texts, test_labels, transform=transform)\n",
        "\n",
        "        #Split data\n",
        "        p_traindata, u_traindata, pos_size, unl_size = get_PUDataSplits(train_dataset)\n",
        "        p_validdata, u_validdata, _, _ = get_PUDataSplits(test_dataset)\n",
        "\n",
        "        p_trainloader = torch.utils.data.DataLoader(p_traindata, batch_size=16, \\\n",
        "            shuffle=True)\n",
        "        u_trainloader = torch.utils.data.DataLoader(u_traindata, batch_size=16, \\\n",
        "            shuffle=True)\n",
        "        p_validloader = torch.utils.data.DataLoader(p_validdata, batch_size=16, \\\n",
        "            shuffle=True)\n",
        "        u_validloader = torch.utils.data.DataLoader(u_validdata, batch_size=16, \\\n",
        "            shuffle=True)\n",
        "\n",
        "        ## Initialize model \n",
        "        net = get_model(net_type)\n",
        "        net = net.to(device)\n",
        "    \n",
        "    return p_trainloader, u_trainloader, p_validloader, u_validloader, net, pos_size, unl_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaOIxnJ1rz38"
      },
      "source": [
        "# Now the Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbOGqitUsHjt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "def sigmoid_loss(out, y): \n",
        "    loss = out.gather(1, 1- y.unsqueeze(1)).mean()\n",
        "    return loss\n",
        "\n",
        "\n",
        "def validate(epoch, net, u_validloader, criterion, device, threshold, logistic=False, show_bar=True, separate=False):\n",
        "    \n",
        "    if show_bar:     \n",
        "        print('\\nTest Epoch: %d' % epoch)\n",
        "    \n",
        "    net.eval() \n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pos_correct = 0\n",
        "    neg_correct = 0\n",
        "\n",
        "    pos_total = 0\n",
        "    neg_total = 0\n",
        "\n",
        "    if (not logistic) and (criterion is None): \n",
        "        # print(\"here\")\n",
        "        criterion = sigmoid_loss\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (_, inputs, targets) in enumerate(u_validloader):\n",
        "            \n",
        "            inputs = inputs.to(device)\n",
        "            outputs = net(inputs[:,:,0]).to_tuple()[0]\n",
        "            \n",
        "\n",
        "            predicted  = torch.nn.functional.softmax(outputs, dim=-1)[:,0] \\\n",
        "                    <= torch.tensor([threshold]).to(device)\n",
        "\n",
        "            if not logistic: \n",
        "                outputs = torch.nn.functional.softmax(outputs, dim=-1)\n",
        "                \n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            total += outputs.size(0)\n",
        "            \n",
        "            correct_preds = predicted.eq(targets).cpu().numpy()\n",
        "            correct += np.sum(correct_preds)\n",
        "\n",
        "            if separate: \n",
        "\n",
        "                true_numpy = targets.cpu().numpy().squeeze()\n",
        "                pos_idx = np.where(true_numpy==0)[0]\n",
        "                neg_idx = np.where(true_numpy==1)[0]\n",
        "\n",
        "                pos_correct += np.sum(correct_preds[pos_idx])\n",
        "                neg_correct += np.sum(correct_preds[neg_idx])\n",
        "\n",
        "                pos_total += len(pos_idx)\n",
        "                neg_total += len(neg_idx)\n",
        "\n",
        "    \n",
        "    if not separate: \n",
        "        return 100.*correct/total\n",
        "    else: \n",
        "        return 100.*correct/total, 100.*pos_correct/pos_total, 100.*neg_correct/neg_total\n",
        "\n",
        "\n",
        "\n",
        "def train(epoch, net, p_trainloader, u_trainloader, optimizer, criterion, device, show_bar=True):\n",
        "    \n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, ( p_data, u_data ) in enumerate(zip(p_trainloader, u_trainloader)):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        #Pull from the data loaders\n",
        "        _, p_inputs, p_targets = p_data\n",
        "        _, u_inputs, u_targets = u_data\n",
        "\n",
        "        #To device\n",
        "        p_targets = p_targets.to(device)\n",
        "        u_targets = u_targets.to(device)\n",
        "\n",
        "        #Cat together into a single input/target set\n",
        "        inputs =  torch.cat((p_inputs, u_inputs), dim=0)\n",
        "        targets =  torch.cat((p_targets, u_targets), dim=0)\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        #Get outputs from BERT\n",
        "        outputs = net((inputs[:,:,0])).to_tuple()[0]\n",
        "\n",
        "        #Break into p and u again\n",
        "        p_outputs = outputs[:len(p_targets)]\n",
        "        u_outputs = outputs[len(p_targets):]\n",
        "        \n",
        "        #Separate losses\n",
        "        p_loss = criterion(p_outputs, p_targets)\n",
        "        u_loss = criterion(u_outputs, u_targets)\n",
        "        \n",
        "        #Average these\n",
        "        loss = (p_loss + u_loss)/2.0\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        #Predictions are just the max across the two classes\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        \n",
        "        #eq returns a True/False if element-wise equal to vals in eq(targets)\n",
        "        correct_preds = predicted.eq(targets).cpu().numpy()\n",
        "        correct += np.sum(correct_preds)\n",
        "\n",
        "    return 100.*correct/total\n",
        "\n",
        "\n",
        "def train_PU_discard(epoch, net,  p_trainloader, u_trainloader, optimizer, criterion, device, keep_sample=None, show_bar=True):\n",
        "    \n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    total_p_loss = 0\n",
        "    total_u_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, ( p_data, u_data ) in enumerate(zip(p_trainloader, u_trainloader)):\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        _, p_inputs, p_targets = p_data\n",
        "        u_index, u_inputs, u_targets = u_data\n",
        "\n",
        "        u_idx = np.where(keep_sample[u_index.numpy()]==1)[0] #This is a key line!\n",
        "\n",
        "        if len(u_idx) <1: \n",
        "            continue\n",
        "\n",
        "        u_targets = u_targets[u_idx]\n",
        "\n",
        "        p_targets = p_targets.to(device)\n",
        "        u_targets = u_targets.to(device)\n",
        "        \n",
        "\n",
        "        u_inputs = u_inputs[u_idx]        \n",
        "        inputs =  torch.cat((p_inputs, u_inputs), dim=0)\n",
        "        targets =  torch.cat((p_targets, u_targets), dim=0)\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        outputs = net(inputs[:,:,0]).to_tuple()[0]\n",
        "\n",
        "        p_outputs = outputs[:len(p_targets)]\n",
        "        u_outputs = outputs[len(p_targets):]\n",
        "        \n",
        "        p_loss = criterion(p_outputs, p_targets)\n",
        "        u_loss = criterion(u_outputs, u_targets)\n",
        "\n",
        "        loss = (p_loss + u_loss)/2.0\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_p_loss += p_loss.item()\n",
        "        total_u_loss += u_loss.item()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        \n",
        "        correct_preds = predicted.eq(targets).cpu().numpy()\n",
        "        correct += np.sum(correct_preds)\n",
        "        \n",
        "    #Save model\n",
        "    model_name = f'best_model_epoch_{epoch}.pt'\n",
        "    root_model_path = os.path.join('/content/gdrive/MyDrive/Capstone/classification/overall_tifa/hu_models_2', model_name)\n",
        "    model_dict = net.state_dict()\n",
        "    state_dict = {'model': model_dict, 'optimizer': optimizer.state_dict()}\n",
        "    torch.save(state_dict, root_model_path)\n",
        "\n",
        "    return 100.*correct/total, total_p_loss, total_u_loss\n",
        "\n",
        "def rank_inputs(_, net, u_trainloader, device, alpha, u_size):\n",
        "\n",
        "    net.eval() \n",
        "    output_probs = np.zeros(u_size)\n",
        "    keep_samples = np.ones_like(output_probs)\n",
        "    true_targets_all = np.zeros(u_size)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_num, (idx, inputs, _) in enumerate(u_trainloader):\n",
        "            \n",
        "            idx = idx.numpy()\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = net(inputs[:,:,0]).to_tuple()[0]\n",
        "            probs  = torch.nn.functional.softmax(outputs, dim=-1)[:,0]      # This is one because our pos are labeled 1\n",
        "            output_probs[idx] = probs.detach().cpu().numpy().squeeze()\n",
        "\n",
        "    sorted_idx = np.argsort(output_probs)\n",
        "    keep_samples[sorted_idx[u_size - int(alpha*u_size):]] = 0\n",
        "    \n",
        "    return keep_samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bhs1AYeDuSab"
      },
      "source": [
        "#Estimator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IqxHVPOuRs7"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import sys \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def p_probs(net, device, p_loader): \n",
        "    net.eval()\n",
        "    pp_probs = None\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (_, inputs, targets) in enumerate(p_loader):\n",
        "           \n",
        "            inputs = inputs.to(device)\n",
        "            outputs = net(inputs[:,:,0]).to_tuple()[0]\n",
        "            #print('p_prob outputs:', outputs) \n",
        "\n",
        "            probs = torch.nn.functional.softmax(outputs, dim=-1)[:,0] \n",
        "\n",
        "#             probs = torch.stack((probs, 1-probs), dim=1)\n",
        "#             probs = probs.to(torch.int32)\n",
        "            if pp_probs is None: \n",
        "                pp_probs = probs.detach().cpu().numpy().squeeze()\n",
        "            else:\n",
        "                pp_probs = np.concatenate((pp_probs, \\\n",
        "                    probs.detach().cpu().numpy().squeeze()), axis=0)\n",
        "    \n",
        "    return pp_probs    \n",
        "\n",
        "def u_probs(net, device, u_loader):\n",
        "    net.eval()\n",
        "    pu_probs = None\n",
        "    pu_targets = None\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (_, inputs, targets) in enumerate(u_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = net(inputs[:,:,0]).to_tuple()[0]\n",
        "\n",
        "            probs = torch.nn.functional.softmax(outputs, dim=-1) \n",
        "\n",
        "            if pu_probs is None: \n",
        "                pu_probs = probs.detach().cpu().numpy().squeeze()\n",
        "                pu_targets = targets.numpy().squeeze()\n",
        "                \n",
        "            else:\n",
        "                pu_probs = np.concatenate( (pu_probs, \\\n",
        "                    probs.detach().cpu().numpy().squeeze()))\n",
        "                pu_targets = np.concatenate( (pu_targets, \\\n",
        "                    targets.numpy().squeeze()))\n",
        "  \n",
        "    return pu_probs, pu_targets\n",
        "\n",
        "def DKW_bound(x,y,t,m,n,delta=0.1, gamma= 0.01):\n",
        "\n",
        "    temp = np.sqrt(np.log(1/delta)/2/n) + np.sqrt(np.log(1/delta)/2/m)\n",
        "    bound = temp*(1+gamma)/(y/n)\n",
        "\n",
        "    estimate = t\n",
        "\n",
        "    return estimate, t - bound, t + bound\n",
        "\n",
        "\n",
        "def BBE_estimator(pdata_probs, udata_probs, udata_targets):\n",
        "\n",
        "    p_indices = np.argsort(pdata_probs)\n",
        "    sorted_p_probs = pdata_probs[p_indices]\n",
        "\n",
        "    #This only returns the positive probabilities\n",
        "    u_indices = np.argsort(udata_probs[:,0])\n",
        "    sorted_u_probs = udata_probs[:,0][u_indices]\n",
        "    sorted_u_targets = udata_targets[u_indices]\n",
        "\n",
        "    #This reverses the order of array\n",
        "    #These are now sorted from largest to smallest\n",
        "    sorted_u_probs = sorted_u_probs[::-1]\n",
        "    sorted_p_probs = sorted_p_probs[::-1]\n",
        "    sorted_u_targets = sorted_u_targets[::-1]\n",
        "    num = len(sorted_u_probs)\n",
        "    estimate_arr = []\n",
        "\n",
        "    #Now, collecting confidencebounds\n",
        "    upper_cfb = []\n",
        "    lower_cfb = []            \n",
        "\n",
        "    i = 0\n",
        "    j = 0\n",
        "\n",
        "    #Num: the number of unlabeled samples\n",
        "    while (i < num):\n",
        "\n",
        "        #This is looping through the sorted u_probs from largest to smallest\n",
        "        start_interval =  sorted_u_probs[i]   \n",
        "\n",
        "        #Pass if index less than total length, and the val is greater than the next val\n",
        "        if (i<num-1 and start_interval> sorted_u_probs[i+1]): \n",
        "            #If this is the case, continue through loop\n",
        "            pass\n",
        "        else: \n",
        "            #If this is not the case, add 1 and break this iteration \n",
        "            i += 1\n",
        "            continue\n",
        "        \n",
        "        \"\"\"\n",
        "        Calc q_hat_p\n",
        "        \"\"\"\n",
        "\n",
        "        while ( j<len(sorted_p_probs) and sorted_p_probs[j] >= start_interval):\n",
        "            j+= 1\n",
        "\n",
        "        if j>1 and i > 1:\n",
        "\n",
        "            #Note that the estimate t accounts for i\n",
        "            t = (i)*1.0*len(sorted_p_probs)/j/len(sorted_u_probs)\n",
        "            estimate, lower , upper = DKW_bound(i, j, t, len(sorted_u_probs), len(sorted_p_probs))\n",
        "            estimate_arr.append(estimate)\n",
        "            upper_cfb.append(upper)\n",
        "            lower_cfb.append(lower)\n",
        "        i+=1\n",
        "\n",
        "    if (len(upper_cfb) != 0): \n",
        "\n",
        "        #This returns the estimate producing the lowest upper-bound\n",
        "        idx = np.argmin(upper_cfb)\n",
        "        mpe_estimate = estimate_arr[idx]\n",
        "        print('mpe_estimate', mpe_estimate)\n",
        "        return mpe_estimate, lower_cfb, upper_cfb\n",
        "    else: \n",
        "        return 0.0, 0.0, 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4zkKiexg-ft"
      },
      "source": [
        "# Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Vg8mWwTg_4I"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_acc_alph(alpha_est, accuracy):\n",
        "\n",
        "    e = len(alpha_est)\n",
        "    x_axis = np.arange(1, e + 1, 1)\n",
        "    plt.figure()\n",
        "    plt.plot(x_axis, alpha_est, label=\"Alpha Est\")\n",
        "    plt.plot(x_axis, accuracy, label=\"Accuracy\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.title(\"Accuracy and Alpha Estimates\")\n",
        "    plt.savefig(\"Acc_alpha_plot.png\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_losses(p_losses, u_losses, alpha_est, accuracy):\n",
        "    e = len(p_losses)\n",
        "    x_axis = np.arange(1, e + 1, 1)\n",
        "    plt.figure()\n",
        "    plt.plot(x_axis, p_losses, label=\"Positive Loss\")\n",
        "    plt.plot(x_axis, u_losses, label=\"Unlabeled Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.title(\"Train until P and U loss Converges\")\n",
        "    plt.savefig(\"loss_plot.png\")\n",
        "    plt.show()\n",
        "\n",
        "    e = len(alpha_est)\n",
        "    x_axis = np.arange(1, e + 1, 1)\n",
        "    plt.figure()\n",
        "    plt.plot(x_axis, alpha_est, label=\"Alpha Est\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.title(\"Alpha Estimates\")\n",
        "    plt.savefig(\"Acc_alpha_plot.png\")\n",
        "    plt.show()\n",
        "\n",
        "    e = len(alpha_est)\n",
        "    x_axis = np.arange(1, e + 1, 1)\n",
        "    plt.figure()\n",
        "    plt.plot(x_axis, accuracy, label=\"Accuracy\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.title(\"Accuracy Estimates\")\n",
        "    plt.savefig(\"Acc_alpha_plot.png\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize model and data"
      ],
      "metadata": {
        "id": "nLTU4Kz6J8xb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOEknqs2tGSP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import argparse\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from transformers import AdamW\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "lr = 2e-7\n",
        "wd = 5e-4\n",
        "momentum = 0.9\n",
        "seed = 42\n",
        "\n",
        "net_type = \"Roberta\"\n",
        "data_type = \"TiFa_BERT\"\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('device count', torch.cuda.device_count())\n",
        "train_method = \"TEDn\"\n",
        "\n",
        "## Train set for positive and unlabeled\n",
        "alpha = 0.5\n",
        "beta = 0.15\n",
        "warm_start = True\n",
        "warm_start_epochs = 5\n",
        "batch_size = 8\n",
        "epochs= 5000\n",
        "log_dir = './TiFa_BERT'\n",
        "optimizer_str='AdamW'\n",
        "alpha_estimate=0.1\n",
        "show_bar = False\n",
        "use_alpha = False\n",
        "estimate_alpha = True\n",
        "load_model = False\n",
        "model_name = 'Roberta'\n",
        "if train_method == \"TEDn\": \n",
        "    use_alpha=True\n",
        "\n",
        "#################\n",
        "\n",
        "## Obtain dataset \n",
        "p_trainloader, u_trainloader, p_validloader, u_validloader, net, train_pos_size, train_unl_size = \\\n",
        "    get_dataset(data_dir, data_type, net_type, device, batch_size)\n",
        "\n",
        "if load_model == True:\n",
        "    best_checkpoint = torch.load(os.path.join(experiment_dir, model_name))\n",
        "    net.load_state_dict(best_checkpoint['model'])            \n",
        "    net = net.to(device)\n",
        "\n",
        "    if optimizer_str==\"AdamW\": \n",
        "      optimizer = AdamW(net.parameters(), lr=lr)\n",
        "      optimizer.load_state_dict(best_checkpoint['optimizer'])\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    net = net.cuda()\n",
        "    criterion = nn.CrossEntropyLoss().cuda()\n",
        "else:\n",
        "  print('failed to place net and criterion on cuda')\n",
        "\n",
        "if load_model == False:\n",
        "  if optimizer_str==\"SGD\":\n",
        "      optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay=wd)\n",
        "  elif optimizer_str==\"Adam\":\n",
        "      optimizer = optim.Adam(net.parameters(), lr=lr,weight_decay=wd)\n",
        "  elif optimizer_str==\"AdamW\": \n",
        "      optimizer = AdamW(net.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Warm-Up Epochs to establish baseline estimate of positive labeled proportion"
      ],
      "metadata": {
        "id": "HYi1tglBKJP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha_estimate=0.1\n",
        "## Train in the begining for warm start\n",
        "\n",
        "if warm_start and train_method==\"TEDn\": \n",
        "  print('employing warm start')\n",
        "\n",
        "  alph_est = []\n",
        "  train_acc_list = []\n",
        "\n",
        "  for epoch in range(warm_start_epochs): \n",
        "      \n",
        "      train_acc = train(epoch, net, p_trainloader, u_trainloader, \\\n",
        "              optimizer=optimizer, criterion=criterion, device=device, show_bar=show_bar)\n",
        "\n",
        "      train_acc_list.append(train_acc)\n",
        "\n",
        "      if estimate_alpha: \n",
        "          pos_probs = p_probs(net, device, p_validloader)\n",
        "          unlabeled_probs, unlabeled_targets = u_probs(net, device, u_validloader)\n",
        "\n",
        "\n",
        "          our_mpe_estimate, _, _ = BBE_estimator(pos_probs, unlabeled_probs, unlabeled_targets)\n",
        "\n",
        "          alpha_estimate =our_mpe_estimate\n",
        "          alph_est.append(alpha_estimate)\n",
        "          print(\"current alpha estimate:\", alpha_estimate)\n",
        "\n",
        "      plot_acc_alph(alph_est, train_acc_list)"
      ],
      "metadata": {
        "id": "Qgxm2nD9rvwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now, run epochs employing Warm-up estimated alpha"
      ],
      "metadata": {
        "id": "w77jvxyiKwdP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Zfws_pxRMPP"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "epochs = 40\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "if train_method=='CVIR' or train_method==\"TEDn\": \n",
        "\n",
        "    print('now, no warm start.')\n",
        "    alpha_used = alpha_estimate\n",
        "\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    #Store losses\n",
        "    p_loss_list = []\n",
        "    u_loss_list = []\n",
        "    epoch_list = []\n",
        "    total_loss_list = []\n",
        "    acc_list = []\n",
        "\n",
        "    alpha_est = []\n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        if use_alpha: \n",
        "            alpha_used =  alpha_estimate\n",
        "        else:\n",
        "            alpha_used = alpha\n",
        "        \n",
        "        keep_samples = rank_inputs(epoch, net, u_trainloader, device,\\\n",
        "             alpha_used, u_size=train_unl_size)\n",
        "\n",
        "        train_acc, p_loss, u_loss = train_PU_discard(epoch, net,  p_trainloader, u_trainloader,\\\n",
        "            optimizer, criterion, device, keep_sample=keep_samples,show_bar=show_bar)\n",
        "        \n",
        "        print('train_acc', train_acc)\n",
        "\n",
        "        total_loss = p_loss + u_loss\n",
        "\n",
        "        acc_list.append(train_acc/100)\n",
        "\n",
        "        epoch_list.append(epoch)\n",
        "        p_loss_list.append(p_loss)\n",
        "        u_loss_list.append(u_loss)\n",
        "        total_loss_list.append(total_loss)\n",
        "\n",
        "        if estimate_alpha: \n",
        "            pos_probs = p_probs(net, device, p_validloader)\n",
        "            unlabeled_probs, unlabeled_targets = u_probs(net, device, u_validloader)\n",
        "            our_mpe_estimate, _, _ = BBE_estimator(pos_probs, unlabeled_probs, unlabeled_targets)\n",
        "\n",
        "            print('Current estimate after', epoch, 'rounds:', our_mpe_estimate)\n",
        "            alpha_estimate = our_mpe_estimate\n",
        "            alpha_est.append(alpha_estimate)\n",
        "            print('{} minutes this epoch'.format(round((time.time() - t0)/60), 2))\n",
        "            t0 = time.time()\n",
        "\n",
        "        plot_losses(p_loss_list, u_loss_list, alpha_est, acc_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJHvQjL6PS8s"
      },
      "source": [
        "# Define inference-tailored data loader and inference function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZyeQdPZ90Fb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from ast import literal_eval\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class ArticleDataset(Dataset):\n",
        "\n",
        "    \n",
        "    def __init__(self, dataframe, tokenizer, max_len, get_wids, inference):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.inference = inference\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # GET TEXT AND WORD LABELS \n",
        "        text = self.data.para_split[index]\n",
        "\n",
        "        if self.inference == False:    \n",
        "          label = self.data.label[index]  \n",
        "        \n",
        "        article_id = self.data.cnki_id[index]\n",
        "\n",
        "        # TOKENIZE TEXT\n",
        "        encoding = self.tokenizer(text, \n",
        "                                padding='max_length', \n",
        "                                truncation=True, \n",
        "                                max_length=510)\n",
        " \n",
        "        # CONVERT TO TORCH TENSORS\n",
        "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
        "        if self.inference == False:\n",
        "          item['label'] = torch.as_tensor(label)\n",
        "        \n",
        "        item['cnki_id'] = article_id\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "\n",
        "def inference_data_processing(data_dir, model_name):\n",
        "\n",
        "\n",
        "  tokenizer = getBertTokenizer('hfl/chinese-roberta-wwm-ext')\n",
        "  \n",
        "  #Pull in dataset\n",
        "  df = pd.read_csv(data_dir)\n",
        "\n",
        "  #Keep original\n",
        "  df['idx'] = df.index.values\n",
        "\n",
        "  # CREATE TRAIN SUBSET AND VALID SUBSET\n",
        "  infer_dataset = df[['cnki_id', 'para_split']]\n",
        "\n",
        "\n",
        "  # tokenizer = AutoTokenizer.from_pretrained(config_data['model']['transformer_path']) \n",
        "  infer_set = ArticleDataset(infer_dataset, tokenizer, 510, False, True)\n",
        "\n",
        "  infer_loader = DataLoader(infer_set,\n",
        "                            batch_size=8,\n",
        "                            shuffle=False, num_workers=2,\n",
        "                            pin_memory=True)\n",
        "\n",
        "  return infer_loader, infer_dataset\n",
        "\n",
        "\n",
        "def inference(infer_loader, model_type, exp_dir, model_loc, device, threshold):\n",
        "    \n",
        "    #Load model\n",
        "    net = get_model(model_type)\n",
        "    best_checkpoint = torch.load(os.path.join(experiment_dir, model_loc))\n",
        "    net.load_state_dict(best_checkpoint['model'])            \n",
        "    net = net.to(device)\n",
        "    net.eval() \n",
        "\n",
        "    loss_list = []\n",
        "    label_list = []\n",
        "    pred_list = []\n",
        "    cnki_list = []\n",
        "    logit_list = []\n",
        "  \n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(infer_loader):\n",
        "\n",
        "            ids = batch['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
        "            cnki_ids = batch['cnki_id']\n",
        "\n",
        "            outputs = net(ids, attention_mask=mask, return_dict=False)[0]\n",
        "\n",
        "            logits = torch.nn.functional.softmax(outputs, dim=-1)[:,0]\n",
        "            predicted  = (logits \\\n",
        "                    <= torch.tensor([threshold]).to(device)).cpu().numpy() \n",
        "\n",
        "            logit_list.append(logits)\n",
        "            cnki_list.append(cnki_ids)\n",
        "            pred_list.append(predicted)\n",
        "\n",
        "    return pred_list, cnki_list, logit_list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initialize inference data and model"
      ],
      "metadata": {
        "id": "_hDTH-TXLcSp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zae1mqD1SpYm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import argparse\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from transformers import AdamW\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "lr = 5e-6\n",
        "wd = 5e-4\n",
        "momentum = 0.9\n",
        "seed = 42\n",
        "\n",
        "net_type = \"Roberta\"\n",
        "data_type = \"TiFa_BERT\"\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('device count', torch.cuda.device_count())\n",
        "train_method = \"TEDn\"\n",
        "\n",
        "alpha = 0.01\n",
        "beta = 0.15\n",
        "warm_start = True\n",
        "warm_start_epochs = 1\n",
        "batch_size = 8\n",
        "epochs= 5000\n",
        "log_dir = './TiFa_BERT'\n",
        "optimizer_str='AdamW'\n",
        "alpha_estimate=0.005\n",
        "show_bar = False\n",
        "use_alpha = False\n",
        "estimate_alpha = True\n",
        "load_model = False\n",
        "if train_method == \"TEDn\": \n",
        "    use_alpha=True\n",
        "\n",
        "infer_loader, infer_df = inference_data_processing(data_dir, model_name='Roberta')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Perform inference"
      ],
      "metadata": {
        "id": "uerts_eeLxxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_list, cnki_list, logit_list = inference(infer_loader, 'Roberta', experiment_dir, 'best_model.pt', device, 0.5)"
      ],
      "metadata": {
        "id": "e8BFEEViL0nY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Align inference with original dataframe"
      ],
      "metadata": {
        "id": "91M-i-TcMicQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwRa74lXOTYk"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(data_dir)\n",
        "cnki_all = [x for y in cnki_list for x in y]\n",
        "pred_all = [x for y in pred_list for x in y]\n",
        "#logit_all = [x for y in logits for x in y]\n",
        "\n",
        "df['inf_lab'] = pred_all\n",
        "df['inf_lab'] = df.inf_lab.map({True:0, False:1})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqeEHNERVsm5"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"hu_inference.csv\", index = False)\n",
        "!mv '/content/hu_inference.csv' '/content/gdrive/MyDrive/Capstone/classification/hu_inference_final.csv'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "name": "loyalty_tedn_implementation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}